{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole-V0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaxFord1/botreplacement/blob/master/CartPole_V0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jeWo3dVKp4R",
        "colab_type": "text"
      },
      "source": [
        "To save video, we need X screen, suddenly colab is a cloud-service and we don't have screen at all, so we need to simulate it with xvfb lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKFroPYH3Z1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12rI-xMcLBKs",
        "colab_type": "text"
      },
      "source": [
        "Creating virtual screen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cVSMWJ23f1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "68ff3121-c511-4696-c4b7-be5ce7317247"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1009'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1009'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUTrkqSMLIzB",
        "colab_type": "text"
      },
      "source": [
        "**Importing all needs `gym, pytorch, numpy`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuGWVHiuKNpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW2LJhWVLUdQ",
        "colab_type": "text"
      },
      "source": [
        "***Setings for our NN***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30cSBLpzKTlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "PERCENTILE = 70"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8rpskKILnpV",
        "colab_type": "text"
      },
      "source": [
        "Our Neural Network, here we have Sequential structure with 2 Linear **layers**. Input defined by our environment's observation and output defined by agent's number of actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRzNWI-rKVGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74V6p3P5MH0I",
        "colab_type": "text"
      },
      "source": [
        "Creating our own classes \"**Episode**\" and \"**EpisodeStep**\". ***EpisodeStep*** class will contain each *`observation`* and token *`action`*. ***Episode*** will contain all *`EpisodeStep`* records and it's `earned *total rewards*`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEFKBET5LmcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsMe6gFaNJqJ",
        "colab_type": "text"
      },
      "source": [
        "Method iterate_bathces takes positional arguments: our environment,  our neural network and batch_size we want to get. The output is list of **` Episode`** class with random actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRDPzy4EKX3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_batches(env, net, batch_size):\n",
        "    batch = []\n",
        "    episode_reward = 0.0\n",
        "    episode_steps = []\n",
        "    obs = env.reset()\n",
        "    sm = nn.Softmax(dim=1)\n",
        "    while True:\n",
        "        obs_v = torch.FloatTensor([obs])\n",
        "        act_probs_v = sm(net(obs_v))\n",
        "        act_probs = act_probs_v.data.numpy()[0]\n",
        "        action = np.random.choice(len(act_probs), p=act_probs)\n",
        "        next_obs, reward, is_done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
        "        if is_done:\n",
        "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
        "            episode_reward = 0.0\n",
        "            episode_steps = []\n",
        "            next_obs = env.reset()\n",
        "            if len(batch) == batch_size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "        obs = next_obs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWuPnGmlN9KH",
        "colab_type": "text"
      },
      "source": [
        "Method filter_batch takes positional arguments: **batches** and **percintile**. Here we define *mean reward* of all batch,  %*percentile* of all given trials . Filters all results and return only best (>percentile). Returns **observations** , **acts**, **%percentile**, **mean reward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCvTyfhcKcsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_batch(batch, percentile):\n",
        "    rewards = list(map(lambda s: s.reward, batch))\n",
        "    reward_bound = np.percentile(rewards, percentile)\n",
        "    reward_mean = float(np.mean(rewards))\n",
        "\n",
        "    train_obs = []\n",
        "    train_act = []\n",
        "    for example in batch:\n",
        "        if example.reward < reward_bound:\n",
        "            continue\n",
        "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
        "        train_act.extend(map(lambda step: step.action, example.steps))\n",
        "\n",
        "    train_obs_v = torch.FloatTensor(train_obs)\n",
        "    train_act_v = torch.LongTensor(train_act)\n",
        "    return train_obs_v, train_act_v, reward_bound, reward_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI9Q2TxOPjpP",
        "colab_type": "text"
      },
      "source": [
        "Creating **environment** for training. Define ***`number`***  of **actions** and **observations** for this game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcfQ2IzwKhmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "obs_size = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdXeq20qQLJL",
        "colab_type": "text"
      },
      "source": [
        "Defining our **NN** with its **optimizer** and **activator** functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEfPpmvmKjul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "sm = nn.Softmax(dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VwNpfOJQvht",
        "colab_type": "text"
      },
      "source": [
        "Training our model for the best result (win is 195, but maximum actions we can do is 200, so until we get reward equal 200)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePPYkCf52rkj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a8754156-d612-4d1d-b380-17ce2b7aea1c"
      },
      "source": [
        "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
        "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
        "    #all gradients = 0\n",
        "    optimizer.zero_grad()\n",
        "    #getting our predictions(2 vals) with given observations\n",
        "    action_scores_v = net(obs_v)\n",
        "    #Measuring losses between predictions and best acts we get  \n",
        "    loss_v = objective(action_scores_v, acts_v)\n",
        "    #measuring all gradients\n",
        "    loss_v.backward()\n",
        "    #optimising our NN\n",
        "    optimizer.step()\n",
        "    #printing info each episode\n",
        "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
        "        iter_no, loss_v.item(), reward_m, reward_b))\n",
        "    if reward_m > 199:\n",
        "        print(\"Solved!\")\n",
        "        break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: loss=0.701, reward_mean=20.5, reward_bound=23.0\n",
            "1: loss=0.660, reward_mean=16.9, reward_bound=19.0\n",
            "2: loss=0.655, reward_mean=17.8, reward_bound=19.0\n",
            "3: loss=0.687, reward_mean=19.4, reward_bound=24.5\n",
            "4: loss=0.672, reward_mean=21.5, reward_bound=22.5\n",
            "5: loss=0.681, reward_mean=19.1, reward_bound=21.5\n",
            "6: loss=0.663, reward_mean=26.5, reward_bound=28.5\n",
            "7: loss=0.657, reward_mean=30.7, reward_bound=38.5\n",
            "8: loss=0.648, reward_mean=41.9, reward_bound=43.5\n",
            "9: loss=0.634, reward_mean=38.1, reward_bound=49.5\n",
            "10: loss=0.622, reward_mean=35.6, reward_bound=43.0\n",
            "11: loss=0.631, reward_mean=35.4, reward_bound=34.0\n",
            "12: loss=0.625, reward_mean=35.9, reward_bound=41.0\n",
            "13: loss=0.615, reward_mean=41.1, reward_bound=44.0\n",
            "14: loss=0.611, reward_mean=44.5, reward_bound=46.5\n",
            "15: loss=0.587, reward_mean=44.7, reward_bound=56.0\n",
            "16: loss=0.589, reward_mean=52.4, reward_bound=58.5\n",
            "17: loss=0.576, reward_mean=55.2, reward_bound=58.0\n",
            "18: loss=0.593, reward_mean=62.6, reward_bound=71.0\n",
            "19: loss=0.575, reward_mean=43.4, reward_bound=50.5\n",
            "20: loss=0.589, reward_mean=52.0, reward_bound=56.0\n",
            "21: loss=0.558, reward_mean=58.4, reward_bound=64.0\n",
            "22: loss=0.571, reward_mean=53.9, reward_bound=62.5\n",
            "23: loss=0.559, reward_mean=69.8, reward_bound=78.5\n",
            "24: loss=0.537, reward_mean=50.7, reward_bound=59.0\n",
            "25: loss=0.519, reward_mean=60.3, reward_bound=70.0\n",
            "26: loss=0.527, reward_mean=61.1, reward_bound=74.0\n",
            "27: loss=0.563, reward_mean=69.2, reward_bound=63.5\n",
            "28: loss=0.522, reward_mean=63.1, reward_bound=75.0\n",
            "29: loss=0.537, reward_mean=62.4, reward_bound=72.5\n",
            "30: loss=0.522, reward_mean=66.6, reward_bound=71.0\n",
            "31: loss=0.536, reward_mean=62.8, reward_bound=64.5\n",
            "32: loss=0.519, reward_mean=85.8, reward_bound=102.0\n",
            "33: loss=0.504, reward_mean=67.2, reward_bound=74.0\n",
            "34: loss=0.547, reward_mean=84.8, reward_bound=100.0\n",
            "35: loss=0.497, reward_mean=65.9, reward_bound=76.0\n",
            "36: loss=0.534, reward_mean=78.1, reward_bound=81.5\n",
            "37: loss=0.507, reward_mean=71.5, reward_bound=78.0\n",
            "38: loss=0.521, reward_mean=84.9, reward_bound=103.5\n",
            "39: loss=0.517, reward_mean=77.4, reward_bound=82.0\n",
            "40: loss=0.503, reward_mean=76.9, reward_bound=84.5\n",
            "41: loss=0.496, reward_mean=93.2, reward_bound=110.5\n",
            "42: loss=0.509, reward_mean=96.4, reward_bound=100.0\n",
            "43: loss=0.508, reward_mean=67.2, reward_bound=72.5\n",
            "44: loss=0.512, reward_mean=85.1, reward_bound=105.0\n",
            "45: loss=0.510, reward_mean=91.4, reward_bound=95.0\n",
            "46: loss=0.493, reward_mean=94.0, reward_bound=111.5\n",
            "47: loss=0.524, reward_mean=94.1, reward_bound=109.0\n",
            "48: loss=0.523, reward_mean=101.3, reward_bound=117.5\n",
            "49: loss=0.502, reward_mean=133.2, reward_bound=148.5\n",
            "50: loss=0.520, reward_mean=123.7, reward_bound=137.5\n",
            "51: loss=0.519, reward_mean=140.3, reward_bound=151.0\n",
            "52: loss=0.514, reward_mean=141.0, reward_bound=167.0\n",
            "53: loss=0.503, reward_mean=118.7, reward_bound=119.5\n",
            "54: loss=0.500, reward_mean=120.9, reward_bound=135.5\n",
            "55: loss=0.522, reward_mean=138.7, reward_bound=162.0\n",
            "56: loss=0.533, reward_mean=133.8, reward_bound=146.5\n",
            "57: loss=0.502, reward_mean=170.1, reward_bound=200.0\n",
            "58: loss=0.506, reward_mean=178.9, reward_bound=200.0\n",
            "59: loss=0.523, reward_mean=187.2, reward_bound=200.0\n",
            "60: loss=0.515, reward_mean=195.5, reward_bound=200.0\n",
            "61: loss=0.510, reward_mean=182.0, reward_bound=200.0\n",
            "62: loss=0.509, reward_mean=187.2, reward_bound=200.0\n",
            "63: loss=0.518, reward_mean=186.1, reward_bound=200.0\n",
            "64: loss=0.514, reward_mean=187.2, reward_bound=200.0\n",
            "65: loss=0.509, reward_mean=195.3, reward_bound=200.0\n",
            "66: loss=0.512, reward_mean=193.7, reward_bound=200.0\n",
            "67: loss=0.514, reward_mean=194.5, reward_bound=200.0\n",
            "68: loss=0.504, reward_mean=187.8, reward_bound=200.0\n",
            "69: loss=0.504, reward_mean=200.0, reward_bound=200.0\n",
            "Solved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCnsxOT_SQNl",
        "colab_type": "text"
      },
      "source": [
        "Creating our environment with screen for **recording**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h08Rnofs2yv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBkecPXiSfJa",
        "colab_type": "text"
      },
      "source": [
        "Method play_n_times makes a trials, where we deploy our NN. Positional args: Environment and number of trials. Returns only reward from each trial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Juzo2d3RkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_n_times(env, net, n):\n",
        "  history = list()\n",
        "  \n",
        "  for i in range(n):\n",
        "    rewards = 0\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "      #NN gets only tensor-like information\n",
        "      obs = torch.FloatTensor([obs])\n",
        "      #getting predictions from our NN\n",
        "      probs = net(obs)\n",
        "      #using activation layer on results\n",
        "      act_probs_v = sm(probs)\n",
        "      #converting to numpy data-type\n",
        "      act_probs_v.data.numpy()[0]\n",
        "      act_probs = act_probs_v.data.numpy()[0]\n",
        "      #getting index of highest prediction\n",
        "      act_probs.max()\n",
        "      if act_probs.max()== act_probs[0]:\n",
        "        action = 0\n",
        "      else:\n",
        "        action = 1\n",
        "      next_obs, reward, is_done, _ = env.step(action)\n",
        "      rewards += reward\n",
        "      done = is_done\n",
        "      obs = next_obs\n",
        "      \n",
        "    history.append(rewards)\n",
        "    \n",
        "  return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80VXmsZrTP2h",
        "colab_type": "text"
      },
      "source": [
        "Starting our agent for interacting with environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEBYfXbP4RZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = play_n_times(env, net, 10)\n",
        "history = np.array(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oN74mfzWTk69",
        "colab_type": "text"
      },
      "source": [
        "Analyzing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsPcBOrY6FKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "946f091e-beea-4715-ac6c-a14b101bfa8c"
      },
      "source": [
        "minimum = history.min()\n",
        "maximum = history.max()\n",
        "mean = history.mean()\n",
        "print(\"Minimal agent's performance = %d; Maximum agent's performance = %d; Mean agent's performance = %d; Extra = %d\" % (minimum, maximum, mean, (minimum+maximum)/(2*mean)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimal agent's performance = 200; Maximum agent's performance = 200; Mean agent's performance = 200; Extra = 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}